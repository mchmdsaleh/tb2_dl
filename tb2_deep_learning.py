# -*- coding: utf-8 -*-
"""tb2_deep_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H0EmtA029MSxolrvbM3wVlzxtdT7nHt2

## Download Dataset
"""

!mkdir data
!mkdir data/fashion
!wget -O data/fashion/train-images-idx3-ubyte.gz http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
!wget -O data/fashion/train-labels-idx1-ubyte.gz http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
!wget -O data/fashion/t10k-images-idx3-ubyte.gz http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
!wget -O data/fashion/t10k-labels-idx1-ubyte.gz http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz

# Import library yang dibutuhkan
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import gzip
from sklearn.model_selection import train_test_split

# Specify the file paths for the dataset
train_images_path = 'data/fashion/train-images-idx3-ubyte.gz'
train_labels_path = 'data/fashion/train-labels-idx1-ubyte.gz'
test_images_path = 'data/fashion/t10k-images-idx3-ubyte.gz'
test_labels_path = 'data/fashion/t10k-labels-idx1-ubyte.gz'

# Read the dataset files into NumPy arrays
def read_mnist_images(filepath):
    with gzip.open(filepath, 'rb') as f:
        data = np.frombuffer(f.read(), np.uint8, offset=16)
    return data.reshape(-1, 28, 28)

def read_mnist_labels(filepath):
    with gzip.open(filepath, 'rb') as f:
        data = np.frombuffer(f.read(), np.uint8, offset=8)
    return data

# Load the dataset
train_images = read_mnist_images(train_images_path)
train_labels = read_mnist_labels(train_labels_path)
test_images = read_mnist_images(test_images_path)
test_labels = read_mnist_labels(test_labels_path)

# Check the shape of the data
print("Training images shape:", train_images.shape)
print("Training labels shape:", train_labels.shape)
print("Test images shape:", test_images.shape)
print("Test labels shape:", test_labels.shape)

# Check the number of classes
num_classes = len(np.unique(train_labels))
print("Number of classes:", num_classes)

# Look at a few sample images from the dataset
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.imshow(train_images[i], cmap='gray')
    plt.title(f"Label: {train_labels[i]}")
    plt.axis('off')
plt.show()

# Visualize the distribution of labels
sns.set(style='darkgrid')
plt.figure(figsize=(8, 5))
sns.countplot(train_labels)
plt.title("Label Distribution")
plt.xlabel("Label")
plt.ylabel("Count")
plt.show()

"""### **Melakukan Pemeriksaan Data**"""

train_images_1d = train_images.reshape(len(train_images), -1)
test_images_1d = test_images.reshape(len(test_images), -1)

# Create a DataFrame from the data
df_train = pd.DataFrame({'image': train_images_1d.tolist(), 'label': train_labels.tolist()})
df_test = pd.DataFrame({'image': test_images_1d.tolist(), 'label': test_labels.tolist()})

# Check for missing values (NaN) in the DataFrame
print("Missing values in training set:")
print(df_train.isnull().sum())

print("\nMissing values in test set:")
print(df_test.isnull().sum())

# Check the data types of each column in the DataFrame
print("\nData types of training set:")
print(df_train.dtypes)

print("\nData types of test set:")
print(df_test.dtypes)

# Check the unique values in the 'label' column
print("\nUnique labels in training set:")
print(df_train['label'].unique())

print("\nUnique labels in test set:")
print(df_test['label'].unique())

"""### **Visualisasi Data**"""

# Correlation plot to visualize the correlation between two variables
plt.figure(figsize=(8, 6))
sns.heatmap(df_train[['image', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Plot')
plt.show()

# Bar chart to display the count of data in a specific variable
# Count the occurrences of each label
unique_labels, label_counts = np.unique(df_train['label'], return_counts=True)

# Create a bar chart to display the count of data for each clothing category
plt.figure(figsize=(8, 6))
sns.barplot(x=unique_labels, y=label_counts)
plt.title('Count of Data for Each Clothing Category')
plt.xlabel('Clothing Category')
plt.ylabel('Count')
plt.show()

from sklearn.model_selection import train_test_split

# Separate the features (X) and the target variable (y)
X = train_images
y = train_labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

n_row = 1
n_col = 5
plt.figure(figsize=(10,8))
for i in list(range(n_row*n_col)):
    plt.subplot(n_row, n_col, i+1)
    plt.imshow(X_train[i,:].reshape(28,28), cmap="gray")
    title_text = "Image" + str(i+1)
    plt.title(title_text, size=6.5)

plt.show()

"""# Preprocessing Normalized all of the datasets"""

from sklearn.preprocessing import StandardScaler

# Reshape the training and test data
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)

# Apply StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

n_row = 1
n_col = 5
plt.figure(figsize=(10,8))
for i in list(range(n_row*n_col)):
    plt.subplot(n_row, n_col, i+1)
    plt.imshow(X_train[i,:].reshape(28,28), cmap="gray")
    title_text = "Image" + str(i+1)
    plt.title(title_text, size=6.5)

plt.show()

"""# Find eigenvectors and eigenvalues, compute covariance matrix"""

mean_vec = np.mean(X_train, axis=0)
cov_mat = np.cov(X_train.T)
eig_vals, eig_vecs = np.linalg.eig(cov_mat)
print("Shape of Covariance matrix", cov_mat.shape)
eig_pairs = [(np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]
eig_pairs.sort(key = lambda x: x[0], reverse=True)
tot = sum(eig_vals)
var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)]
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance

add =0.0
for i in range(400):
    add+= var_exp[i]

print('first 400 components average variance: %.2f' % add)

size = 400
plt.figure(figsize=(6,4))
plt.bar(range(size), var_exp[0:size], align='center');
plt.xlabel("Principal components")
plt.ylabel("Explained variance ratio");

"""# Using PCA reduce dimensionality"""

from sklearn.decomposition import PCA

n_components = 400
pca = PCA(n_components=n_components)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.fit_transform(X_test)

eigenvalues = pca.components_.reshape(n_components, 28, 28)
eigenvalues = pca.components_
print("Eigenvalues shape:", eigenvalues.shape)

n_row = 1
n_col = 5
plt.figure(figsize=(10,8))
for i in list(range(n_row*n_col)):
    plt.subplot(n_row, n_col, i+1)
    plt.imshow(eigenvalues[i,:].reshape(28,28), cmap="gray")
    title_text = "Image" + str(i+1)
    plt.title(title_text, size=6.5)

plt.show()

"""# Classification"""

from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn import metrics

"""# Gaussian classifier"""

gNB = GaussianNB()
gNB.fit(X_train_pca,y_train)
nb_predict = gNB.predict(X_test_pca)

print(metrics.classification_report(y_test, nb_predict))
accuracy = metrics.accuracy_score(y_test, nb_predict)
average_accuracy = np.mean(y_test == nb_predict) * 100
print("The average_accuracy is {0:.1f}%".format(average_accuracy))

"""# SVM classifier with default parameters"""

svm = SVC()
svm.fit(X_train_pca,y_train)
svm_predict = svm.predict(X_test_pca)

print(metrics.classification_report(y_test, svm_predict))
accuracy = metrics.accuracy_score(y_test, svm_predict)
average_accuracy = np.mean(y_test == svm_predict) * 100
print("The average_accuracy is {0:.1f}%".format(average_accuracy))

"""# MLP classifier with default parameters"""

MLP= MLPClassifier()
MLP.fit(X_train_pca,y_train)
mlp_predict = MLP.predict(X_test_pca)

print(metrics.classification_report(y_test, mlp_predict))
accuracy = metrics.accuracy_score(y_test, mlp_predict)
average_accuracy = np.mean(y_test == mlp_predict) * 100
print("The average_accuracy is {0:.1f}%".format(average_accuracy))

"""# Preprocessing binarizer"""

from sklearn.preprocessing import Binarizer
binarizer = Binarizer()
train_binary = binarizer.fit_transform(X_train)
test_binary = binarizer.fit_transform(X_test)
train_binary.shape

n_row = 1
n_col = 5
plt.figure(figsize=(10,8))
for i in list(range(n_row*n_col)):
    plt.subplot(n_row, n_col, i+1)
    plt.imshow(train_binary[i,:].reshape(28,28), cmap="binary")
    title_text = "Image" + str(i+1)
    plt.title(title_text, size=6.5)

plt.show()

"""# Gaussian classifier"""

gNB = GaussianNB()
gNB.fit(train_binary,y_train)
nb_predict = gNB.predict(test_binary)

print(metrics.classification_report(y_test, nb_predict))
accuracy = metrics.accuracy_score(y_test, nb_predict)
average_accuracy = np.mean(y_test == nb_predict) * 100
print("The average_accuracy is {0:.1f}%".format(average_accuracy))

svm = SVC()
svm.fit(train_binary,y_train)
svm_predict = svm.predict(test_binary)

print(metrics.classification_report(y_test, svm_predict))
accuracy = metrics.accuracy_score(y_test, svm_predict)
average_accuracy = np.mean(y_test == svm_predict) * 100
print("The average_accuracy is {0:.1f}%".format(average_accuracy))

"""# MLP we use only one layer and 100 neurons

"""

MLP= MLPClassifier()
MLP.fit(train_binary,y_train)
mlp_predict = MLP.predict(test_binary)

print(metrics.classification_report(y_test, mlp_predict))
accuracy = metrics.accuracy_score(y_test, mlp_predict)
average_accuracy = np.mean(y_test == mlp_predict) * 100
print("The average_accuracy is {0:.1f}%".format(average_accuracy))

